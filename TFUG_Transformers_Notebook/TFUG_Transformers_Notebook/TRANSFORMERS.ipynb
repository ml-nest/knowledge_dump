{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png'>\n",
    "<p style='text-align:center'><b>Author: </b> Tamoghna Saha</p>\n",
    "\n",
    "# ðŸ¤— Introduction\n",
    "\n",
    "__ðŸ¤— Transformers__ provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, etc.) for __Natural Language Understanding (NLU)__ and __Natural Language Generation (NLG)__ with over 2000+ pre-trained models in 100+ languages available in __TensorFlow 2.0__ and __PyTorch__, with a seamless integration between them, allowing you to train your models with one then load it for inference with the other.\n",
    "\n",
    "ðŸ¤— Transformers provide APIs to quickly download and use those pre-trained models on a given text, fine-tune them on your own datasets then share them with the community on our [model hub](https://huggingface.co/models).\n",
    "\n",
    "## Why should I use ðŸ¤— Transformers?\n",
    "\n",
    "1. _Easy-to-use state-of-the-art models_:\n",
    "    * High performance on NLU and NLG tasks.\n",
    "    * Low barrier to entry for educators and practitioners.\n",
    "    * Few user-facing abstractions with just three classes to learn.\n",
    "    * A unified API for using all our pre-trained models.\n",
    "    \n",
    "2. _Lower compute costs, smaller carbon footprint_:\n",
    "    * Researchers can share trained models instead of always retraining.\n",
    "    * Practitioners can reduce compute time and production costs.\n",
    "    * Dozens of architectures with over 2,000 pre-trained models, some in more than 100 languages.\n",
    "    \n",
    "3. _Choose the right framework for every part of a model's lifetime_:\n",
    "    * Train state-of-the-art models in 3 lines of code.\n",
    "    * Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "    * Seamlessly pick the right framework for training, evaluation, production.\n",
    "    \n",
    "4. _Easily customize a model or an example to your needs_:\n",
    "    * Examples for each architecture to reproduce the results by the official authors of said architecture.\n",
    "    * Expose the models internal as consistently as possible.\n",
    "    * Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ðŸ¤— Transformers provides the following tasks out of the box:\n",
    "\n",
    "* Sentiment analysis\n",
    "* Text generation (in English)\n",
    "* Name entity recognition (NER)\n",
    "* Question answering\n",
    "* Filling masked text\n",
    "* Summarization\n",
    "* Translation\n",
    "\n",
    "<details><summary>By translation, I didn't mean this...</summary>\n",
    "<img src='https://i.pinimg.com/originals/83/42/b6/8342b62b4cdbbb32e05f107348bbc69d.gif'>\n",
    "So let's help our beloved Joey ðŸ¤— in translation.\n",
    "</details>\n",
    "\n",
    "## What was the need for ðŸ¤— Transformers?\n",
    "\n",
    "__Recurrent neural networks (RNN)__ are capable of looking at previous inputs to predict the next possible word. But RNNâ€™s curse of the __shorter window of reference__, resulting in Vanishing Gradient, makes it difficult to capture the context of a story when the story gets longer. This is still true for __Gated Recurrent Units (GRUâ€™s)__ and __Long-short Term Memory (LSTMâ€™s)__ networks, although they do have a bigger capacity to achieve longer-term memory compared to RNN.\n",
    "\n",
    "Not only that, __RNN is slow to train__. Such a recurrent process does not make use of modern graphics processing units (GPUs), which were designed for parallel computation. But what's even worse is that __LSTM is even slower to train__.\n",
    "\n",
    "The attention mechanism, __in theory__, have an infinite window to reference from, therefore being capable of using the entire context of the story. In terms of training, Transformers is definitely faster because of the parallel processing capability. Let's find out more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T15:32:34.232816Z",
     "start_time": "2020-10-25T15:32:34.229083Z"
    }
   },
   "source": [
    "# ðŸ¤— Transformers Architecture\n",
    "\n",
    "## High-level look\n",
    "\n",
    "<img src='./source/the_transformer.png'>\n",
    "<img src='./source/down_arrow.png' width=\"100\" height=\"100\">\n",
    "<img src='./source/the_transformer_encoders_decoders.png'>\n",
    "<img src='./source/down_arrow.png' width=\"100\" height=\"100\">\n",
    "<img src='./source/the_transformer_encoder_decoder_stack.png'>\n",
    "<img src='./source/down_arrow.png' width=\"100\" height=\"100\">\n",
    "<img src='./source/transformer_encoder_decoder_detail.png'>\n",
    "\n",
    "I believe this looks familiar and \"professional\" to you!\n",
    "\n",
    "<img src='./source/transformer_model_architecture.png' width=\"400\" height=\"400\">\n",
    "\n",
    "The breakdown of this \"professional\" diagram is like this!\n",
    "\n",
    "<img src='./source/transformer_breakdown.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder - in depth!\n",
    "\n",
    "Now, we will deep dive into the Encoder section. This is the \"professional\" view.\n",
    "\n",
    "<img src='./source/transformer_encoder_architecture.png' width=400 height=400>\n",
    "\n",
    "We pass a sentence as an input (of course), but machine can only understand 0s and 1s (again, of course).\n",
    "\n",
    "<details><summary>So we need to translate the words in a sentence into...</summary>\n",
    "<img src='./source/The-Matrix.jpg' width=200 height=200>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "This model is trained from corpus of ~30,000 unique words. Each of these words have a unique ID, known as vocabulary index.\n",
    "<img src='./source/vocab.png' width=175 height=175>\n",
    "So, the words in the sentence are transformed into it's corresponding vocabulary indices.\n",
    "<img src='./source/converted_tokens.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embedding\n",
    "\n",
    "The next step is to convert the input word into it's corresponding word embedding. _Word embedding is the vector representation of each words in the vocabulary._\n",
    "<img src='./source/embedding.png'>\n",
    "For simplicity in explanation, I used 3 dimension `d` over here, but in reality, it is __512, 768__ or even __1024__. The more, the better.\n",
    "\n",
    "Each of these dimensions captures \"some\" linguistic feature about that word. Since the model decides these features itself during the training, it can be non-trivial to find out what exactly each of the dimensions represents.\n",
    "\n",
    "These vectors are randomly initialized and IT IS THESE that will get fine-tuned during the model training, and will ultimately generate the contextual representation of the words to be leveraged during the inference time.\n",
    "<img src='./source/before-after_embed.png' width=450 height=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "In recurrent networks like LSTMs and GRUs, the network processes the input sequentially, token after token. The hidden state at position `t+1` depends on the hidden state from position `t`. This way, the network has a reference to identify the relative positions of each token by accumulating information. However, __Transformers has no notion of word order. That's why it is faster but we do need the information of word's positions.__\n",
    "\n",
    "<p style='color:red; text-align: center'><b>Here is why position matters!</b></p>\n",
    "<img src='./source/order_matters.png' width=450 height=450>\n",
    "\n",
    "__Hence the requirement of positional encoding.__\n",
    "\n",
    "<p style='color:blue; text-align: center; font-size: 20px'><b>So how do we do it?</b></p>\n",
    "\n",
    "> __Strategy 1__: Add vector of positions IDs `(0,1,2,...,(N-1))` with the Word Vectors.\n",
    "<img src='./source/pos_encode_strategy_1.png' width=450 height=450>\n",
    "<details><summary>But there is a problem ...</summary>\n",
    "    <p style='color:red'>Adding numbers like these will distort the word embedding value, specially those of the ones appearing in the later part of the text.</p>\n",
    "</details>\n",
    "\n",
    "> __Strategy 2__: Add vector of fractions of positions IDs (`0*1/(N-1)`, `1*1/(N-1)`, `2*1/(N-1)`,...,`(N-1)*1/(N-1)`) with the Word Vectors.\n",
    "<img src='./source/pos_encode_strategy_2.png' width=450 height=450>\n",
    "<details><summary>But there is a problem ...</summary>\n",
    "    <p style='color:red'>Different sentences will have different number of words - so even when we try to get fractional value for those positional vectors, it will be <b>different for the same position for different sentences</b>. This positional vectors needs to be constant for their corresponding position.</p>\n",
    "</details>\n",
    "\n",
    "<b style='color:green'>Implemented Strategy</b>\n",
    "\n",
    "Hence, the authors propose a cyclic (dynamic) solution where __sine and cosine function with different frequencies__ is added to each word embedding. The formula goes like this:\n",
    "\n",
    "$$\\begin{gather*}\n",
    "PE_{(pos, 2i)} = \\sin \\left({\\frac{pos}{10000^{\\frac{2i}{d_{model}} } }} \\right)\n",
    "\\end{gather*}$$\n",
    "$$\\begin{gather*}\n",
    "PE_{(pos, 2i+1)} = \\cos \\left({\\frac{pos}{10000^{\\frac{2i}{d_{model}} } }} \\right)\n",
    "\\end{gather*}$$\n",
    "\n",
    "Let us try understanding the `sin` part of the formula to compute the position embeddings:\n",
    "\n",
    "<img src='./source/pos_encode_sin_1.png' width=450 height=450>\n",
    "\n",
    "Here `pos` refers to the position of the __word__ in the sequence. `P0` refers to the position embedding of the first word, `d` means the size of the word/token embedding (here it is d=5). Finally, `i` refers to each of the 5 individual dimensions of the embedding (i.e. 0, 1,2,3,4).\n",
    "\n",
    "While `d` is fixed, `pos` and `i` vary. Let us try understanding the latter two.\n",
    "\n",
    "__`pos`__\n",
    "\n",
    "<img src='./source/pos_encode_sin_2.png' width=450 height=450>\n",
    "\n",
    "If we plot a sin curve and vary `pos` (on the x-axis), you will land up with different position values on the y-axis. Therefore, words with different positions will have different values position embeddings values.\n",
    "\n",
    "There is a problem though. Since `sin` curve repeat in intervals, you can see in the figure above that `P0` and `P6` have the same position embedding values, despite being at two very different (word) positions. This is where the `i` part in the equation comes into play.\n",
    "\n",
    "__`i`__\n",
    "\n",
    "<img src='./source/pos_encode_sin_3.png' width=450 height=450>\n",
    "\n",
    "If you vary `i` in the equation above, you will get a bunch of curves with varying frequencies. Reading off the position embedding values against different frequencies, lands up giving different values at different embedding dimensions for `P0` and `P6`.\n",
    "\n",
    "For every <span style='color:red;'><b>odd index</b></span> on the position vector, we pass the <span style='color:red;'><b>cosine function</b></span> and for every <span style='color:blue;'><b>even index</b></span>, the <span style='color:blue;'><b>sine function</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(word_pos, d_model):\n",
    "    # get the matrix of word pos and angle rate based on index of positional vector\n",
    "    angle_rads = get_angles(np.arange(word_pos)[:, np.newaxis], \n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    # final positional encoded vector\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Position Vector Index (X): %{x}<br>Word Index (Y): %{y}<br>Position Vector Value (Z): %{z}<extra></extra>",
         "type": "heatmap",
         "y": [
          "I",
          "am",
          "a",
          "student",
          "from",
          "Kolkata"
         ],
         "z": [
          [
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           0,
           1
          ],
          [
           0.8414709848078965,
           0.6479058722668407,
           0.6815613503552693,
           0.7964578744859591,
           0.5331684399140229,
           0.8837559606641002,
           0.4093089240419384,
           0.934061592402767,
           0.31098359290718575,
           0.962739013512927,
           0.23492107560384184,
           0.9789891657304084,
           0.17689218624615002,
           0.9881665438469512,
           0.13295726553038986,
           0.9933397990439348,
           0.09983341664682815,
           0.9962528714565815,
           0.07491915794147602,
           0.9978922583284672,
           0.056204499214692484,
           0.9988145474375343,
           0.042157153217256826,
           0.9993333133756003,
           0.03161750640243371,
           0.9996250763256367,
           0.023711514583386627,
           0.9997891591576791,
           0.01778185687966613,
           0.9998814336577873,
           0.01333481909619642,
           0.9999333246693383,
           0.009999833334166664,
           0.9999625055238417,
           0.007498871810771602,
           0.9999789152489235,
           0.005623383613960186,
           0.9999881431549026,
           0.004216952536060113,
           0.9999933324002487,
           0.0031622723897082473,
           0.9999962505312964,
           0.0023713714831265597,
           0.9999978915182238,
           0.001778278472803529,
           0.9999988143133814,
           0.0013335210369344083,
           0.999999333239358,
           0.0009999998333333417,
           0.9999996250529187,
           0.0007498941390497072,
           0.9999997891517557,
           0.0005623412955523594,
           0.999999881431317,
           0.0004216964909303456,
           0.9999999333239291,
           0.0003162277607463752,
           0.9999999625052898,
           0.00023713736834362982,
           0.9999999789151749,
           0.00017782794006665674,
           0.9999999881431315,
           0.00013335214282110343,
           0.9999999933323929,
           9.999999983333334e-05,
           0.9999999962505289,
           7.498942086296283e-05,
           0.9999999978915175,
           5.6234132489396926e-05,
           0.9999999988143131,
           4.2169650330359994e-05,
           0.9999999993332392,
           3.162277659641333e-05,
           0.9999999996250529,
           2.3713737054394017e-05,
           0.9999999997891518,
           1.7782794099451994e-05,
           0.9999999998814313,
           1.3335214321238012e-05,
           0.9999999999333239,
           9.999999999833334e-06,
           0.9999999999625053,
           7.498942093254276e-06,
           0.9999999999789152,
           5.6234132518738535e-06,
           0.9999999999881432,
           4.2169650342733244e-06,
           0.9999999999933324,
           3.162277660163109e-06,
           0.9999999999962506,
           2.3713737056594326e-06,
           0.9999999999978915,
           1.7782794100379854e-06,
           0.9999999999988143,
           1.3335214321629287e-06,
           0.9999999999993332,
           9.999999999998333e-07,
           0.9999999999996251,
           7.498942093323855e-07,
           0.9999999999997892,
           5.623413251903194e-07,
           0.9999999999998814,
           4.216965034285697e-07,
           0.9999999999999333,
           3.162277660168326e-07,
           0.9999999999999625,
           2.371373705661633e-07,
           0.9999999999999789,
           1.7782794100389134e-07,
           0.9999999999999881,
           1.33352143216332e-07,
           0.9999999999999933,
           9.999999999999982e-08,
           0.9999999999999962,
           7.498942093324552e-08,
           0.9999999999999979,
           5.6234132519034885e-08,
           0.9999999999999988,
           4.216965034285821e-08,
           0.9999999999999993,
           3.1622776601683785e-08,
           0.9999999999999997,
           2.3713737056616548e-08,
           0.9999999999999998,
           1.7782794100389228e-08,
           0.9999999999999999,
           1.333521432163324e-08,
           0.9999999999999999
          ],
          [
           0.9092974268256817,
           -0.16043596136428848,
           0.9974799976053368,
           0.2686902916613834,
           0.9021307149638974,
           0.5620491960186534,
           0.7469035352242571,
           0.7449421168039855,
           0.5911271172152932,
           0.8537328162796878,
           0.4566933595230835,
           0.9168395732350422,
           0.34820527588995176,
           0.9529462367568571,
           0.263553681039078,
           0.9734479127292895,
           0.19866933079506122,
           0.9850395677709679,
           0.1494172118896749,
           0.9915779184637764,
           0.11223131102926329,
           0.995261000345693,
           0.08423935034568837,
           0.9973341424445114,
           0.06320339793316936,
           0.9985005864380697,
           0.04740969582708172,
           0.9991567255384379,
           0.03555809079063615,
           0.999525762747104,
           0.02666726692410943,
           0.9997333075685526,
           0.01999866669333308,
           0.9998500249070382,
           0.014997321930968175,
           0.9999156618848273,
           0.011246589401385216,
           0.9999525729007798,
           0.008433830083032673,
           0.9999733296899085,
           0.00632451315671895,
           0.9999850021533028,
           0.004742729631057545,
           0.9999915660817866,
           0.0035565513221982523,
           0.9999952572563376,
           0.002667039702496165,
           0.9999973329583212,
           0.0019999986666669333,
           0.9999985002119562,
           0.00149978785640297,
           0.9999991566071117,
           0.0011246824132767918,
           0.9999995257252964,
           0.0008433929068712736,
           0.9999997332957254,
           0.0006324554898699746,
           0.9999998500211619,
           0.00047427472335204553,
           0.9999999156607005,
           0.00035565587450990023,
           0.9999999525725263,
           0.0002667042832708332,
           0.9999999733295715,
           0.0001999999986666667,
           0.9999999850021158,
           0.00014997884130422916,
           0.9999999915660699,
           0.00011246826480096591,
           0.9999999952572526,
           8.433930058573056e-05,
           0.9999999973329572,
           6.324555316120389e-05,
           0.9999999985002116,
           4.742747409545282e-05,
           0.999999999156607,
           3.5565588193280577e-05,
           0.9999999995257253,
           2.667042864010465e-05,
           0.9999999997332957,
           1.9999999998666667e-05,
           0.9999999998500212,
           1.4997884186086856e-05,
           0.9999999999156607,
           1.124682650356988e-05,
           0.9999999999525725,
           8.43393006847166e-06,
           0.9999999999733296,
           6.324555320294596e-06,
           0.9999999999850021,
           4.74274741130553e-06,
           0.9999999999915661,
           3.5565588200703474e-06,
           0.9999999999952572,
           2.667042864323486e-06,
           0.9999999999973329,
           1.9999999999986667e-06,
           0.9999999999985002,
           1.4997884186643495e-06,
           0.9999999999991566,
           1.124682650380461e-06,
           0.9999999999995257,
           8.433930068570644e-07,
           0.9999999999997333,
           6.324555320336337e-07,
           0.99999999999985,
           4.7427474113231323e-07,
           0.9999999999999156,
           3.55655882007777e-07,
           0.9999999999999526,
           2.6670428643266163e-07,
           0.9999999999999734,
           1.9999999999999867e-07,
           0.999999999999985,
           1.4997884186649062e-07,
           0.9999999999999916,
           1.1246826503806958e-07,
           0.9999999999999952,
           8.433930068571634e-08,
           0.9999999999999973,
           6.324555320336754e-08,
           0.9999999999999984,
           4.742747411323308e-08,
           0.9999999999999991,
           3.556558820077845e-08,
           0.9999999999999996,
           2.6670428643266477e-08,
           0.9999999999999998
          ],
          [
           0.1411200080598672,
           -0.8558006752482378,
           0.7782725224195122,
           -0.36845687730268334,
           0.993253167134793,
           0.10967269367180045,
           0.9536344621455892,
           0.45758204733687047,
           0.8126488966420368,
           0.6811047651845121,
           0.652904012444876,
           0.8161628520895868,
           0.5085361344196323,
           0.8951726346490128,
           0.3894703168372061,
           0.9405893089765657,
           0.2955202066613396,
           0.9664441243237718,
           0.22307542478359724,
           0.9810835984004498,
           0.1679033097924294,
           0.9893477838474878,
           0.12617176856958728,
           0.9940051528477726,
           0.0947260913327461,
           0.9966273735330602,
           0.0710812178889303,
           0.9981029658279507,
           0.053323080542305924,
           0.9989330716091481,
           0.039994972637127126,
           0.9993999753700556,
           0.02999550020249566,
           0.9996625665843931,
           0.02249492869372799,
           0.9998102425750742,
           0.016869439541363287,
           0.9998932900811337,
           0.01265055765316364,
           0.9999399921357196,
           0.009486690678650789,
           0.9999662549503698,
           0.00711406110867237,
           0.9999810237173625,
           0.005334812924793146,
           0.9999893288373035,
           0.004000553625316836,
           0.999993999159557,
           0.002999995500002025,
           0.9999966254779559,
           0.0022496807303635817,
           0.9999981023663348,
           0.0016870231753454267,
           0.9999989328820222,
           0.0012650891728333796,
           0.9999993999154155,
           0.0009486831557480256,
           0.9999996625476247,
           0.0007114120516900337,
           0.9999998102365795,
           0.0005334837977063175,
           0.9999998932881852,
           0.0004000564189778156,
           0.9999999399915361,
           0.00029999999550000005,
           0.9999999662547607,
           0.0002249682609021025,
           0.9999999810236574,
           0.000168702396756879,
           0.9999999893288184,
           0.0001265089506911223,
           0.9999999939991535,
           9.48683296627489e-05,
           0.999999996625476,
           7.11412111098412e-05,
           0.9999999981023657,
           5.3348382275862324e-05,
           0.9999999989328818,
           4.000564295422854e-05,
           0.9999999993999154,
           2.9999999995500003e-05,
           0.9999999996625476,
           2.2496826278076043e-05,
           0.9999999998102366,
           1.6870239754910247e-05,
           0.9999999998932881,
           1.2650895102520015e-05,
           0.9999999999399916,
           9.486832980362836e-06,
           0.9999999999662548,
           7.114121116924958e-06,
           0.9999999999810236,
           5.334838230091463e-06,
           0.9999999999893289,
           4.000564296479301e-06,
           0.9999999999939991,
           2.9999999999955002e-06,
           0.9999999999966255,
           2.2496826279954696e-06,
           0.9999999999981024,
           1.6870239755702469e-06,
           0.9999999999989329,
           1.265089510285409e-06,
           0.9999999999993999,
           9.486832980503715e-07,
           0.9999999999996625,
           7.114121116984364e-07,
           0.9999999999998103,
           5.334838230116515e-07,
           0.9999999999998933,
           4.000564296489865e-07,
           0.9999999999999399,
           2.999999999999955e-07,
           0.9999999999999662,
           2.2496826279973487e-07,
           0.999999999999981,
           1.6870239755710393e-07,
           0.9999999999999893,
           1.2650895102857433e-07,
           0.999999999999994,
           9.486832980505123e-08,
           0.9999999999999967,
           7.114121116984959e-08,
           0.9999999999999981,
           5.334838230116766e-08,
           0.9999999999999989,
           4.0005642964899705e-08,
           0.9999999999999994
          ],
          [
           -0.7568024953079282,
           -0.9485206046022331,
           0.14153892328073767,
           -0.8556110543338414,
           0.7784717414604921,
           -0.3682014025095705,
           0.9932807345309828,
           0.10987751477680553,
           0.9535807404869198,
           0.4577194431856943,
           0.81257090773398,
           0.6811896060996285,
           0.6528280017427488,
           0.8162130602981118,
           0.5084713395449978,
           0.895201677594021,
           0.3894183423086505,
           0.9406059001488304,
           0.295479780122797,
           0.9664535367699112,
           0.22304449157464745,
           0.981088917618219,
           0.1678798513825203,
           0.9893507833710578,
           0.12615406653456793,
           0.9940068422343384,
           0.09471276991763454,
           0.9966283243773869,
           0.07107120849969298,
           0.9981035007903601,
           0.053315566231905635,
           0.9989333725239165,
           0.03998933418663416,
           0.9994001446132098,
           0.02999127047961455,
           0.9996626617651443,
           0.022491756224229137,
           0.9998102961017786,
           0.016867060261365977,
           0.9998933201822449,
           0.012648773333754897,
           0.9999400090630818,
           0.00948535258100043,
           0.9999662644694084,
           0.007113057657232752,
           0.9999810290703377,
           0.00533406043403642,
           0.999989331847511,
           0.003999989333341867,
           0.9999940008523236,
           0.00299957233923581,
           0.9999966264298694,
           0.0022493634039305057,
           0.9999981029016353,
           0.0016867852138272863,
           0.999998933183044,
           0.0012649107267577616,
           0.9999994000846926,
           0.0009485493400223825,
           0.9999996626428163,
           0.0007113117040324959,
           0.9999998102901095,
           0.000533408547570677,
           0.9999998933182873,
           0.0003999999893333334,
           0.9999999400084638,
           0.00029995767923488633,
           0.9999999662642799,
           0.00022493652817930828,
           0.9999999810290104,
           0.00016867860057154577,
           0.9999999893318285,
           0.00012649110606942557,
           0.9999999940008463,
           9.485494808422393e-05,
           0.999999996626428,
           7.113117634157384e-05,
           0.9999999981029011,
           5.334085726123831e-05,
           0.9999999989331828,
           3.999999998933334e-05,
           0.9999999994000847,
           2.9995768368800138e-05,
           0.9999999996626427,
           2.2493653005717133e-05,
           0.9999999998102901,
           1.6867860136343403e-05,
           0.9999999998933183,
           1.264911064033621e-05,
           0.9999999999400084,
           9.485494822504379e-06,
           0.9999999999662643,
           7.113117640095707e-06,
           0.9999999999810291,
           5.334085728628001e-06,
           0.9999999999893319,
           3.999999999989333e-06,
           0.9999999999940008,
           2.9995768373253252e-06,
           0.9999999999966265,
           2.2493653007594992e-06,
           0.9999999999981029,
           1.686786013713529e-06,
           0.9999999999989332,
           1.2649110640670143e-06,
           0.9999999999994,
           9.485494822645198e-07,
           0.9999999999996626,
           7.11311764015509e-07,
           0.9999999999998103,
           5.334085728653043e-07,
           0.9999999999998933,
           3.9999999999998934e-07,
           0.99999999999994,
           2.9995768373297785e-07,
           0.9999999999999662,
           2.2493653007613774e-07,
           0.999999999999981,
           1.686786013714321e-07,
           0.9999999999999893,
           1.2649110640673482e-07,
           0.999999999999994,
           9.485494822646606e-08,
           0.9999999999999967,
           7.113117640155685e-08,
           0.9999999999999981,
           5.3340857286532934e-08,
           0.9999999999999989
          ],
          [
           -0.9589242746631385,
           -0.37330346412752385,
           -0.5711272011926853,
           -0.9944594461401601,
           0.32393520361009215,
           -0.7604730620572294,
           0.8588959971593322,
           -0.2523173144935074,
           0.9999465167896046,
           0.20022396521205094,
           0.9267573131721942,
           0.5175916362698149,
           0.7765299800281857,
           0.717936243026044,
           0.6184437125719255,
           0.8378896000735105,
           0.479425538604203,
           0.9077185335407779,
           0.366223309062432,
           0.947749406353272,
           0.2774805305947537,
           0.9705039826461559,
           0.20928944114998235,
           0.983377240026117,
           0.15745589818234362,
           0.990640957540351,
           0.1182910635376447,
           0.9947334230160381,
           0.0887968623598419,
           0.9970372470090952,
           0.0666266789698157,
           0.9983335612519332,
           0.04997916927067833,
           0.9990627786722368,
           0.037485925740325936,
           0.9994729256784466,
           0.02811336165718684,
           0.9997035929308082,
           0.021083262926553093,
           0.9998333144518641,
           0.015810729501231077,
           0.9999062646882548,
           0.011856590713296084,
           0.9999472884001634,
           0.00889127989621496,
           0.999970357975122,
           0.006667557757307569,
           0.9999833310284073,
           0.004999979166692708,
           0.9999906263370276,
           0.003749462261324633,
           0.9999947287983381,
           0.002811702921204439,
           0.9999970357843323,
           0.002108480954863656,
           0.9999983330986729,
           0.0015811381712764261,
           0.9999990626323848,
           0.0011856865750138822,
           0.999999472879417,
           0.0008891395878650232,
           0.9999997035783015,
           0.0006667606666780442,
           0.9999998333098256,
           0.0004999999791666669,
           0.9999999062632253,
           0.00037494709588088416,
           0.9999999472879375,
           0.0002811706588904258,
           0.9999999703578288,
           0.00021084825015201154,
           0.9999999833309822,
           0.00015811388234961114,
           0.9999999906263224,
           0.0001185686850052658,
           0.9999999947287938,
           8.891397038479171e-05,
           0.9999999970357829,
           6.667607155876257e-05,
           0.9999999983330982,
           4.999999997916667e-05,
           0.9999999990626323,
           3.749471045783745e-05,
           0.9999999994728793,
           2.8117066255812708e-05,
           0.9999999997035783,
           2.108482516986683e-05,
           0.9999999998333098,
           1.581138830018309e-05,
           0.9999999999062632,
           1.185686852803046e-05,
           0.9999999999472879,
           8.891397050077458e-06,
           0.9999999999703578,
           6.667607160767216e-06,
           0.999999999983331,
           4.999999999979166e-06,
           0.9999999999906263,
           3.749471046653494e-06,
           0.9999999999947288,
           2.8117066259480403e-06,
           0.9999999999970358,
           2.1084825171413484e-06,
           0.9999999999983331,
           1.5811388300835308e-06,
           0.9999999999990626,
           1.1856868528305498e-06,
           0.9999999999994729,
           8.891397050193442e-07,
           0.9999999999997036,
           6.667607160816126e-07,
           0.9999999999998334,
           4.999999999999791e-07,
           0.9999999999999063,
           3.7494710466621914e-07,
           0.9999999999999473,
           2.8117066259517087e-07,
           0.9999999999999704,
           2.1084825171428956e-07,
           0.9999999999999833,
           1.581138830084183e-07,
           0.9999999999999907,
           1.1856868528308248e-07,
           0.9999999999999948,
           8.891397050194602e-08,
           0.999999999999997,
           6.667607160816615e-08,
           0.9999999999999983
          ]
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "title": {
          "text": "Position Vector"
         }
        },
        "yaxis": {
         "title": {
          "text": "Words"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"bbbcf232-e85c-4f87-b149-86784c962d89\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bbbcf232-e85c-4f87-b149-86784c962d89\")) {                    Plotly.newPlot(                        \"bbbcf232-e85c-4f87-b149-86784c962d89\",                        [{\"hovertemplate\": \"Position Vector Index (X): %{x}<br>Word Index (Y): %{y}<br>Position Vector Value (Z): %{z}<extra></extra>\", \"type\": \"heatmap\", \"y\": [\"I\", \"am\", \"a\", \"student\", \"from\", \"Kolkata\"], \"z\": [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8414709848078965, 0.6479058722668407, 0.6815613503552693, 0.7964578744859591, 0.5331684399140229, 0.8837559606641002, 0.4093089240419384, 0.934061592402767, 0.31098359290718575, 0.962739013512927, 0.23492107560384184, 0.9789891657304084, 0.17689218624615002, 0.9881665438469512, 0.13295726553038986, 0.9933397990439348, 0.09983341664682815, 0.9962528714565815, 0.07491915794147602, 0.9978922583284672, 0.056204499214692484, 0.9988145474375343, 0.042157153217256826, 0.9993333133756003, 0.03161750640243371, 0.9996250763256367, 0.023711514583386627, 0.9997891591576791, 0.01778185687966613, 0.9998814336577873, 0.01333481909619642, 0.9999333246693383, 0.009999833334166664, 0.9999625055238417, 0.007498871810771602, 0.9999789152489235, 0.005623383613960186, 0.9999881431549026, 0.004216952536060113, 0.9999933324002487, 0.0031622723897082473, 0.9999962505312964, 0.0023713714831265597, 0.9999978915182238, 0.001778278472803529, 0.9999988143133814, 0.0013335210369344083, 0.999999333239358, 0.0009999998333333417, 0.9999996250529187, 0.0007498941390497072, 0.9999997891517557, 0.0005623412955523594, 0.999999881431317, 0.0004216964909303456, 0.9999999333239291, 0.0003162277607463752, 0.9999999625052898, 0.00023713736834362982, 0.9999999789151749, 0.00017782794006665674, 0.9999999881431315, 0.00013335214282110343, 0.9999999933323929, 9.999999983333334e-05, 0.9999999962505289, 7.498942086296283e-05, 0.9999999978915175, 5.6234132489396926e-05, 0.9999999988143131, 4.2169650330359994e-05, 0.9999999993332392, 3.162277659641333e-05, 0.9999999996250529, 2.3713737054394017e-05, 0.9999999997891518, 1.7782794099451994e-05, 0.9999999998814313, 1.3335214321238012e-05, 0.9999999999333239, 9.999999999833334e-06, 0.9999999999625053, 7.498942093254276e-06, 0.9999999999789152, 5.6234132518738535e-06, 0.9999999999881432, 4.2169650342733244e-06, 0.9999999999933324, 3.162277660163109e-06, 0.9999999999962506, 2.3713737056594326e-06, 0.9999999999978915, 1.7782794100379854e-06, 0.9999999999988143, 1.3335214321629287e-06, 0.9999999999993332, 9.999999999998333e-07, 0.9999999999996251, 7.498942093323855e-07, 0.9999999999997892, 5.623413251903194e-07, 0.9999999999998814, 4.216965034285697e-07, 0.9999999999999333, 3.162277660168326e-07, 0.9999999999999625, 2.371373705661633e-07, 0.9999999999999789, 1.7782794100389134e-07, 0.9999999999999881, 1.33352143216332e-07, 0.9999999999999933, 9.999999999999982e-08, 0.9999999999999962, 7.498942093324552e-08, 0.9999999999999979, 5.6234132519034885e-08, 0.9999999999999988, 4.216965034285821e-08, 0.9999999999999993, 3.1622776601683785e-08, 0.9999999999999997, 2.3713737056616548e-08, 0.9999999999999998, 1.7782794100389228e-08, 0.9999999999999999, 1.333521432163324e-08, 0.9999999999999999], [0.9092974268256817, -0.16043596136428848, 0.9974799976053368, 0.2686902916613834, 0.9021307149638974, 0.5620491960186534, 0.7469035352242571, 0.7449421168039855, 0.5911271172152932, 0.8537328162796878, 0.4566933595230835, 0.9168395732350422, 0.34820527588995176, 0.9529462367568571, 0.263553681039078, 0.9734479127292895, 0.19866933079506122, 0.9850395677709679, 0.1494172118896749, 0.9915779184637764, 0.11223131102926329, 0.995261000345693, 0.08423935034568837, 0.9973341424445114, 0.06320339793316936, 0.9985005864380697, 0.04740969582708172, 0.9991567255384379, 0.03555809079063615, 0.999525762747104, 0.02666726692410943, 0.9997333075685526, 0.01999866669333308, 0.9998500249070382, 0.014997321930968175, 0.9999156618848273, 0.011246589401385216, 0.9999525729007798, 0.008433830083032673, 0.9999733296899085, 0.00632451315671895, 0.9999850021533028, 0.004742729631057545, 0.9999915660817866, 0.0035565513221982523, 0.9999952572563376, 0.002667039702496165, 0.9999973329583212, 0.0019999986666669333, 0.9999985002119562, 0.00149978785640297, 0.9999991566071117, 0.0011246824132767918, 0.9999995257252964, 0.0008433929068712736, 0.9999997332957254, 0.0006324554898699746, 0.9999998500211619, 0.00047427472335204553, 0.9999999156607005, 0.00035565587450990023, 0.9999999525725263, 0.0002667042832708332, 0.9999999733295715, 0.0001999999986666667, 0.9999999850021158, 0.00014997884130422916, 0.9999999915660699, 0.00011246826480096591, 0.9999999952572526, 8.433930058573056e-05, 0.9999999973329572, 6.324555316120389e-05, 0.9999999985002116, 4.742747409545282e-05, 0.999999999156607, 3.5565588193280577e-05, 0.9999999995257253, 2.667042864010465e-05, 0.9999999997332957, 1.9999999998666667e-05, 0.9999999998500212, 1.4997884186086856e-05, 0.9999999999156607, 1.124682650356988e-05, 0.9999999999525725, 8.43393006847166e-06, 0.9999999999733296, 6.324555320294596e-06, 0.9999999999850021, 4.74274741130553e-06, 0.9999999999915661, 3.5565588200703474e-06, 0.9999999999952572, 2.667042864323486e-06, 0.9999999999973329, 1.9999999999986667e-06, 0.9999999999985002, 1.4997884186643495e-06, 0.9999999999991566, 1.124682650380461e-06, 0.9999999999995257, 8.433930068570644e-07, 0.9999999999997333, 6.324555320336337e-07, 0.99999999999985, 4.7427474113231323e-07, 0.9999999999999156, 3.55655882007777e-07, 0.9999999999999526, 2.6670428643266163e-07, 0.9999999999999734, 1.9999999999999867e-07, 0.999999999999985, 1.4997884186649062e-07, 0.9999999999999916, 1.1246826503806958e-07, 0.9999999999999952, 8.433930068571634e-08, 0.9999999999999973, 6.324555320336754e-08, 0.9999999999999984, 4.742747411323308e-08, 0.9999999999999991, 3.556558820077845e-08, 0.9999999999999996, 2.6670428643266477e-08, 0.9999999999999998], [0.1411200080598672, -0.8558006752482378, 0.7782725224195122, -0.36845687730268334, 0.993253167134793, 0.10967269367180045, 0.9536344621455892, 0.45758204733687047, 0.8126488966420368, 0.6811047651845121, 0.652904012444876, 0.8161628520895868, 0.5085361344196323, 0.8951726346490128, 0.3894703168372061, 0.9405893089765657, 0.2955202066613396, 0.9664441243237718, 0.22307542478359724, 0.9810835984004498, 0.1679033097924294, 0.9893477838474878, 0.12617176856958728, 0.9940051528477726, 0.0947260913327461, 0.9966273735330602, 0.0710812178889303, 0.9981029658279507, 0.053323080542305924, 0.9989330716091481, 0.039994972637127126, 0.9993999753700556, 0.02999550020249566, 0.9996625665843931, 0.02249492869372799, 0.9998102425750742, 0.016869439541363287, 0.9998932900811337, 0.01265055765316364, 0.9999399921357196, 0.009486690678650789, 0.9999662549503698, 0.00711406110867237, 0.9999810237173625, 0.005334812924793146, 0.9999893288373035, 0.004000553625316836, 0.999993999159557, 0.002999995500002025, 0.9999966254779559, 0.0022496807303635817, 0.9999981023663348, 0.0016870231753454267, 0.9999989328820222, 0.0012650891728333796, 0.9999993999154155, 0.0009486831557480256, 0.9999996625476247, 0.0007114120516900337, 0.9999998102365795, 0.0005334837977063175, 0.9999998932881852, 0.0004000564189778156, 0.9999999399915361, 0.00029999999550000005, 0.9999999662547607, 0.0002249682609021025, 0.9999999810236574, 0.000168702396756879, 0.9999999893288184, 0.0001265089506911223, 0.9999999939991535, 9.48683296627489e-05, 0.999999996625476, 7.11412111098412e-05, 0.9999999981023657, 5.3348382275862324e-05, 0.9999999989328818, 4.000564295422854e-05, 0.9999999993999154, 2.9999999995500003e-05, 0.9999999996625476, 2.2496826278076043e-05, 0.9999999998102366, 1.6870239754910247e-05, 0.9999999998932881, 1.2650895102520015e-05, 0.9999999999399916, 9.486832980362836e-06, 0.9999999999662548, 7.114121116924958e-06, 0.9999999999810236, 5.334838230091463e-06, 0.9999999999893289, 4.000564296479301e-06, 0.9999999999939991, 2.9999999999955002e-06, 0.9999999999966255, 2.2496826279954696e-06, 0.9999999999981024, 1.6870239755702469e-06, 0.9999999999989329, 1.265089510285409e-06, 0.9999999999993999, 9.486832980503715e-07, 0.9999999999996625, 7.114121116984364e-07, 0.9999999999998103, 5.334838230116515e-07, 0.9999999999998933, 4.000564296489865e-07, 0.9999999999999399, 2.999999999999955e-07, 0.9999999999999662, 2.2496826279973487e-07, 0.999999999999981, 1.6870239755710393e-07, 0.9999999999999893, 1.2650895102857433e-07, 0.999999999999994, 9.486832980505123e-08, 0.9999999999999967, 7.114121116984959e-08, 0.9999999999999981, 5.334838230116766e-08, 0.9999999999999989, 4.0005642964899705e-08, 0.9999999999999994], [-0.7568024953079282, -0.9485206046022331, 0.14153892328073767, -0.8556110543338414, 0.7784717414604921, -0.3682014025095705, 0.9932807345309828, 0.10987751477680553, 0.9535807404869198, 0.4577194431856943, 0.81257090773398, 0.6811896060996285, 0.6528280017427488, 0.8162130602981118, 0.5084713395449978, 0.895201677594021, 0.3894183423086505, 0.9406059001488304, 0.295479780122797, 0.9664535367699112, 0.22304449157464745, 0.981088917618219, 0.1678798513825203, 0.9893507833710578, 0.12615406653456793, 0.9940068422343384, 0.09471276991763454, 0.9966283243773869, 0.07107120849969298, 0.9981035007903601, 0.053315566231905635, 0.9989333725239165, 0.03998933418663416, 0.9994001446132098, 0.02999127047961455, 0.9996626617651443, 0.022491756224229137, 0.9998102961017786, 0.016867060261365977, 0.9998933201822449, 0.012648773333754897, 0.9999400090630818, 0.00948535258100043, 0.9999662644694084, 0.007113057657232752, 0.9999810290703377, 0.00533406043403642, 0.999989331847511, 0.003999989333341867, 0.9999940008523236, 0.00299957233923581, 0.9999966264298694, 0.0022493634039305057, 0.9999981029016353, 0.0016867852138272863, 0.999998933183044, 0.0012649107267577616, 0.9999994000846926, 0.0009485493400223825, 0.9999996626428163, 0.0007113117040324959, 0.9999998102901095, 0.000533408547570677, 0.9999998933182873, 0.0003999999893333334, 0.9999999400084638, 0.00029995767923488633, 0.9999999662642799, 0.00022493652817930828, 0.9999999810290104, 0.00016867860057154577, 0.9999999893318285, 0.00012649110606942557, 0.9999999940008463, 9.485494808422393e-05, 0.999999996626428, 7.113117634157384e-05, 0.9999999981029011, 5.334085726123831e-05, 0.9999999989331828, 3.999999998933334e-05, 0.9999999994000847, 2.9995768368800138e-05, 0.9999999996626427, 2.2493653005717133e-05, 0.9999999998102901, 1.6867860136343403e-05, 0.9999999998933183, 1.264911064033621e-05, 0.9999999999400084, 9.485494822504379e-06, 0.9999999999662643, 7.113117640095707e-06, 0.9999999999810291, 5.334085728628001e-06, 0.9999999999893319, 3.999999999989333e-06, 0.9999999999940008, 2.9995768373253252e-06, 0.9999999999966265, 2.2493653007594992e-06, 0.9999999999981029, 1.686786013713529e-06, 0.9999999999989332, 1.2649110640670143e-06, 0.9999999999994, 9.485494822645198e-07, 0.9999999999996626, 7.11311764015509e-07, 0.9999999999998103, 5.334085728653043e-07, 0.9999999999998933, 3.9999999999998934e-07, 0.99999999999994, 2.9995768373297785e-07, 0.9999999999999662, 2.2493653007613774e-07, 0.999999999999981, 1.686786013714321e-07, 0.9999999999999893, 1.2649110640673482e-07, 0.999999999999994, 9.485494822646606e-08, 0.9999999999999967, 7.113117640155685e-08, 0.9999999999999981, 5.3340857286532934e-08, 0.9999999999999989], [-0.9589242746631385, -0.37330346412752385, -0.5711272011926853, -0.9944594461401601, 0.32393520361009215, -0.7604730620572294, 0.8588959971593322, -0.2523173144935074, 0.9999465167896046, 0.20022396521205094, 0.9267573131721942, 0.5175916362698149, 0.7765299800281857, 0.717936243026044, 0.6184437125719255, 0.8378896000735105, 0.479425538604203, 0.9077185335407779, 0.366223309062432, 0.947749406353272, 0.2774805305947537, 0.9705039826461559, 0.20928944114998235, 0.983377240026117, 0.15745589818234362, 0.990640957540351, 0.1182910635376447, 0.9947334230160381, 0.0887968623598419, 0.9970372470090952, 0.0666266789698157, 0.9983335612519332, 0.04997916927067833, 0.9990627786722368, 0.037485925740325936, 0.9994729256784466, 0.02811336165718684, 0.9997035929308082, 0.021083262926553093, 0.9998333144518641, 0.015810729501231077, 0.9999062646882548, 0.011856590713296084, 0.9999472884001634, 0.00889127989621496, 0.999970357975122, 0.006667557757307569, 0.9999833310284073, 0.004999979166692708, 0.9999906263370276, 0.003749462261324633, 0.9999947287983381, 0.002811702921204439, 0.9999970357843323, 0.002108480954863656, 0.9999983330986729, 0.0015811381712764261, 0.9999990626323848, 0.0011856865750138822, 0.999999472879417, 0.0008891395878650232, 0.9999997035783015, 0.0006667606666780442, 0.9999998333098256, 0.0004999999791666669, 0.9999999062632253, 0.00037494709588088416, 0.9999999472879375, 0.0002811706588904258, 0.9999999703578288, 0.00021084825015201154, 0.9999999833309822, 0.00015811388234961114, 0.9999999906263224, 0.0001185686850052658, 0.9999999947287938, 8.891397038479171e-05, 0.9999999970357829, 6.667607155876257e-05, 0.9999999983330982, 4.999999997916667e-05, 0.9999999990626323, 3.749471045783745e-05, 0.9999999994728793, 2.8117066255812708e-05, 0.9999999997035783, 2.108482516986683e-05, 0.9999999998333098, 1.581138830018309e-05, 0.9999999999062632, 1.185686852803046e-05, 0.9999999999472879, 8.891397050077458e-06, 0.9999999999703578, 6.667607160767216e-06, 0.999999999983331, 4.999999999979166e-06, 0.9999999999906263, 3.749471046653494e-06, 0.9999999999947288, 2.8117066259480403e-06, 0.9999999999970358, 2.1084825171413484e-06, 0.9999999999983331, 1.5811388300835308e-06, 0.9999999999990626, 1.1856868528305498e-06, 0.9999999999994729, 8.891397050193442e-07, 0.9999999999997036, 6.667607160816126e-07, 0.9999999999998334, 4.999999999999791e-07, 0.9999999999999063, 3.7494710466621914e-07, 0.9999999999999473, 2.8117066259517087e-07, 0.9999999999999704, 2.1084825171428956e-07, 0.9999999999999833, 1.581138830084183e-07, 0.9999999999999907, 1.1856868528308248e-07, 0.9999999999999948, 8.891397050194602e-08, 0.999999999999997, 6.667607160816615e-08, 0.9999999999999983]]}],                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"title\": {\"text\": \"Position Vector\"}}, \"yaxis\": {\"title\": {\"text\": \"Words\"}}},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bbbcf232-e85c-4f87-b149-86784c962d89');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = 'I am a student from Kolkata'\n",
    "splitted_sentence = sentence.split(' ')\n",
    "word_pos_list = len(splitted_sentence)\n",
    "model_dim = 128\n",
    "\n",
    "position_encoded_vector = positional_encoding(word_pos_list, model_dim)\n",
    "\n",
    "trace = go.Heatmap(\n",
    "    z=position_encoded_vector[0], y=splitted_sentence,\n",
    "    hovertemplate='Position Vector Index (X): %{x}<br>Word Index (Y): %{y}<br>Position Vector Value (Z): %{z}<extra></extra>'\n",
    ")\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(xaxis=go.layout.XAxis(\n",
    "    title=go.layout.xaxis.Title(\n",
    "        text='Position Vector',\n",
    "    )),\n",
    "yaxis=go.layout.YAxis(\n",
    "    title=go.layout.yaxis.Title(\n",
    "        text='Words',\n",
    "    )\n",
    "))\n",
    "fig = go.Figure(data, layout=layout)\n",
    "\n",
    "fig.show(config= {'displaylogo': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:purple'><b>Final Step/Summary</b></p>\n",
    "\n",
    "Finally, add these positional vectors to their corresponding input embedding vectors. This successfully gives the network information on the position of each word.\n",
    "\n",
    "<img src='./source/transformer_positional_encoding.png' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T05:13:04.747142Z",
     "start_time": "2020-10-27T05:13:04.740083Z"
    }
   },
   "source": [
    "### Multi-Headed Self-Attention Mechanism\n",
    "\n",
    "#### Self-Attention\n",
    "\n",
    "Self-attention allows the models to associate each word in the input to other words.\n",
    "\n",
    "_Example #1_\n",
    "<img src='./source/self-attention_visual.png' width=500 height=500>\n",
    "\n",
    "_Example #2_\n",
    "\n",
    "Say the following sentence is an input sentence we want to translate:\n",
    "\n",
    "`The animal didn't cross the street because it was too tired`\n",
    "\n",
    "What does `it` in this sentence refer to? Is it referring to `street` or `animal`? As the model processes each word, self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
    "\n",
    "__So how is it working?__\n",
    "<img src='./source/self_attn_full.png' width=350 height=350>\n",
    "\n",
    "<details><summary>What motivated to have this architecture?</summary>\n",
    "    <p>This analogy can be partially motivated in the way retrieval system works.</p>\n",
    "    <img src='./source/q_k.png' width=750 height=750>\n",
    "    <img src='./source/down_arrow.png' width=\"100\" height=\"100\">\n",
    "    <img src='./source/q_k_v.png' width=750 height=750>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1: 3 Linear Components__\n",
    "\n",
    "<img src='./source/self_attn_full_1.png' width=350 height=350>\n",
    "\n",
    "We feed the positional embedding input into __3 distinct linear layers comprising of randomly initialized weight matrix__ to create 3 vectors - <span style='color:purple'><b>Query</b></span>, <span style='color:orange'><b>Key</b></span> and <span style='color:blue'><b>Value</b></span>.\n",
    "\n",
    "<img src='./source/transformer_self_attention_vectors.png'  width=600 height=600>\n",
    "\n",
    "Multiplying <span style='color:green'><b>$X_1$</b></span> by the <span style='color:purple'><b>$W^Q$</b></span> weight matrix produces <span style='color:purple'><b>q1</b></span>, the \"query\" vector associated with that word. Likewise, we end up creating a \"key\" and a \"value\" projection of each word in the input sentence.\n",
    "\n",
    "__NOTE__ : These new vectors are smaller in dimension than the positional embedding vector. We will come back to this.\n",
    "\n",
    "__Step 2 : Getting the Attention Weight__\n",
    "\n",
    "<img src='./source/self_attn_full_2.png' width=350 height=350>\n",
    "\n",
    "<img src='./source/score_matrix.png'>\n",
    "\n",
    "Now, <span style='color:purple'><b>Query</b></span> and __transpose of__ <span style='color:orange'><b>Key</b></span> undergoes dot product matrix multiplication to generate a __score matrix__, which determines how much focus should a word be put on other words. __Higher score means more focus. This is how the queries are mapped to the keys.__\n",
    "\n",
    "<img src='./source/before-after-attention-filter.png'>\n",
    "\n",
    "Then, the scores get scaled down by getting divided by the __square root of the dimension of key vector__. This is to allow for more stable gradients, as multiplying values can have exploding effects. Next, you take the __`softmax`__ of the __scaled score__ to get the __attention weights (or filters)__, which gives you probability values between 0 and 1. \n",
    "\n",
    "By doing a softmax the higher scores get enhanced, and lower scores are depressed. This allows the model to be more confident about which words to attend.\n",
    "\n",
    "__Step 3 : Mapping the Attention Weight with Original Matrix__\n",
    "\n",
    "<img src='./source/self_attn_full_3.png' width=350 height=350>\n",
    "\n",
    "Then you take the __attention weights__ and __multiply__ it by your <span style='color:blue'><b>Value</b></span> vector to get an __output matrix__ <span style='color:pink'><b>Z</b></span>. The higher softmax scores will keep the value of words the model learns is more important. The lower scores will drown out the irrelevant words.\n",
    "\n",
    "<br>\n",
    "<details><summary>Why is this multiplication done?</summary>\n",
    "    <p>The best way to explain the reason for implementing this technique is in the context of computer vision.</p><br><p>Imagine encountering Yahiko from the 6 path of Pain. In reality, the entire view is like this.</p>\n",
    "    <img src='./source/yahiko_original.png'>\n",
    "    <p>But you need to focus on Yahiko.</p>\n",
    "    <img src='./source/yahiko_focused.png'>\n",
    "    <p>This is achieved using the following way.</p>\n",
    "    <img src='./source/yahiko_complete_view.png'>\n",
    "</details>\n",
    "\n",
    "<p style='color:purple'><b>Final Step/Summary</b></p>\n",
    "\n",
    "So, this is how self-attention works! The following formula gives you the summary:\n",
    "\n",
    "<img src='./source/self-attention_matrix_calculation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Headed\n",
    "\n",
    "The paper further refined the self-attention layer by adding a mechanism called __multi-headed__ attention.\n",
    "\n",
    "<img src=\"./source/transformer_multihead_attn.png\" width=250 height=250>\n",
    "\n",
    "Each self-attention process we learned above is called a __head__. Stacking up multiple self-attention will give us multi-headed attention. In the paper, we have __8 heads__. So the `512 d` input gets segmented to 8 `64 d` vectors. In case of BERT, there are 12 `64 d` vectors, resulting in `(12*64=)768 d` vectors.\n",
    "\n",
    "<br>\n",
    "<details><summary>Why is this technique implemented?</summary>\n",
    "    <p>In theory, <b>each head would learn something different</b> therefore giving the encoder model <b>more representation power</b>. Another visual example will help.</p><br>You are encountering all the members of the 6 path of pain.\n",
    "    <img src=\"./source/6_path_of_pain_original.png\">\n",
    "    <p>Now, we decided to process 2 individual at a time to process the entire scenario, thereby keeping an eye on everyone.</p>\n",
    "    <div class=\"row\" style=\"display: flex;\">\n",
    "      <div class=\"column\" style=\"flex: 33.33%; padding: 5px;\">\n",
    "        <img src=\"./source/6_path_of_pain_attn_1.png\" alt=\"attn1\" style=\"width:100%\">\n",
    "      </div>\n",
    "      <div class=\"column\">\n",
    "        <img src=\"./source/6_path_of_pain_attn_2.png\" alt=\"attn2\" style=\"width:100%\">\n",
    "      </div>\n",
    "      <div class=\"column\">\n",
    "        <img src=\"./source/6_path_of_pain_attn_3.png\" alt=\"attn3\" style=\"width:100%\">\n",
    "      </div>\n",
    "    </div>\n",
    "</details>\n",
    "\n",
    "In this multi-headed attention computation, each head has <span style='color:purple'><b>Query</b></span>, <span style='color:orange'><b>Key</b></span> and <span style='color:blue'><b>Value</b></span> weight matrices which are _randomly initialized and mutually exclusive_ that will help to project the positional embeddings into a different representation subspace.\n",
    "\n",
    "<img src='./source/transformer_attention_heads_qkv.png'  width=750 height=750>\n",
    "\n",
    "Now, if we perform the same self-attention calculation as outlined in the previous section, 8 different times with different weight matrices, we end up with 8 different <span style='color:pink'><b>Z</b></span> matrices.\n",
    "\n",
    "<img src='./source/transformer_attention_heads_z.png' width=750 height=750>\n",
    "\n",
    "An example to clearly understand it.\n",
    "\n",
    "<img src='./source/encoder-self-attention-example.png'>\n",
    "\n",
    "However, this leaves us with a bit of a challenge. The upcoming feed-forward neural network (FFNN) layer is not expecting 8 matrices - __itâ€™s expecting a single matrix (a vector for each word)__. Hence, we __concatenate the matrices__, then pass it through another __linear layer__ (again comprising of weights matrices) <span style='color:pink'><b>$W_O$</b></span> and get the original vector dimension back (for example, to 512).\n",
    "\n",
    "<img src='./source/transformer_attention_heads_weight_matrix_o.png' width=750 height=750>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:purple'><b>Recap</b></p>\n",
    "\n",
    "A quick recap of the operations and steps performed in multi-headed self-attention mechanism.\n",
    "\n",
    "<img src='./source/transformer_multi-headed_self-attention-recap.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connections and Layer Normalization\n",
    "\n",
    "The multi-headed attention output vector is added to the original positional input embedding. This is called a __residual connection__. The purpose of this component is to __preserve the original context__, thereby _tackling the Vanishing Gradient problem_.\n",
    "\n",
    "The output of the residual connection goes through a __layer normalization__. This is placed after each sub-layer (self-attention, FFNN) for each encoder.\n",
    "\n",
    "<img src='./source/transformer_resideual_layer_norm.png' width=600 height=600>\n",
    "\n",
    "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons using __batch normalization__. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. So, researchers __transposed batch normalization into layer normalization__.\n",
    "\n",
    "Just to have a better understanding of this, take a look at this visual.\n",
    "\n",
    "<img src='./source/diff_batch_layer_norm.png' width=650 height=650>\n",
    "\n",
    "In batch normalization, the statistics are computed across the batch. In contrast, in layer normalization, the statistics are computed across each feature and are independent of other examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Neural Network\n",
    "\n",
    "The penultimate layer in the block is a pack of feed-forward networks. Each word vector in the sentence (up to the capped sentence length) is given its own feed-forward network. Thus each position in the sentence is learned independently of each other position. This network consists of __2 linear layers *(2 1D convolutions with kernel size 1)* with a ReLU activation in between__.\n",
    "\n",
    "The output of this network is further normalized by first performing residual connection and layer normalization.\n",
    "\n",
    "__But, WHY do we need this layer?__\n",
    "\n",
    "It's __main purpose__ is to process the output from one attention layer in a way to _better fit_ the input for the next attention layer.\n",
    "\n",
    "This layer which usually appear near the end of a network.\n",
    "\n",
    "After the attention layer, the latent representation of each words contains information from other words. However, we want to consolidate a __unique representation__ for each words. This is done using a localized layer, which does not consider neighbors or other positions, and simply transforms the local representation on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:brown; font-size:150%;'><b>Encoder - Wrap up!</b></span>\n",
    "\n",
    "That wraps up the encoder layer. All of these operations are to encode the input to a continuous representation with attention information. This will help the decoder focus on the appropriate words in the input during the decoding process. You can __stack the encoder `N times`__ to further encode the information, where each layer has the opportunity to learn different attention representations therefore potentially boosting the predictive power of the transformer network.\n",
    "\n",
    "Based on this, <span style='color:gold; background-color:black;'><b>BERT</b></span> came into the picture. \n",
    "\n",
    "<br>\n",
    "<details><summary>Not this BERT actually...</summary>\n",
    "<img src='./source/bert_bert.jpg' width=500>\n",
    "</details>\n",
    "\n",
    "A simple example of BERT implementation is done in the image below:\n",
    "\n",
    "<img src='./source/BERT-classification-spam.png'>\n",
    "\n",
    "On September 2020, __Google__ published <span style='color:gold; background-color:black;'><b>BigBird</b></span> (again inspired by __Sesame Street__).\n",
    "\n",
    "<img src='./source/bigbird_architecture.png' width=750>\n",
    "\n",
    "This Transformer based model is expected to handle larger input sequences. It incorporates __Sparse Attention Mechanism__ which enables it to process sequences of length up to __8 times more__ than what was possible with BERT. Using this, researchers decreased the complexity of $O(n^2)$ (of BERT) to just $O(n)$.\n",
    "\n",
    "Link to the paper can be found [here](https://arxiv.org/pdf/2007.14062.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder - in depth!\n",
    "\n",
    "Now, we will deep dive into the Decoder section. This is the \"professional\" view.\n",
    "\n",
    "<img src='./source/transformer_decoder_architecture.png' width=250 height=250>\n",
    "\n",
    "The decoder's job is to generate text sequences. The decoder has a similar sub-layer as the encoder. it has 2 multi-headed attention layers, a pointwise feed-forward layer, residual connections, and layer normalization. These sub-layers behave similarly to the layers in the encoder but __each multi-headed attention layer has a different job__. The decoder is capped off with a linear layer that acts as a classifier, and a softmax to get the word probabilities.\n",
    "\n",
    "__The decoder is autoregressive__. This is how it operates:\n",
    "* It begins with a special token `<start>`.\n",
    "* This token's corresponding vector gets calculated with __encoder outputs that contain the attention information__ and generates a possible word.\n",
    "* Then it takes the __previous output(s) as input(s)__ and again that _encoder outputs_.\n",
    "* Then it generates the next possible word, and this process goes on.\n",
    "* The decoder stops decoding when it generates `<eos>` (short for *end-of-sentence*) token as an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Embedding and Positional Encoding\n",
    "\n",
    "The beginning of the decoder is pretty much the same as the encoder. The input goes through an embedding layer and positional encoding layer to get positional embeddings.\n",
    "\n",
    "### First Multi-Headed Self Attention Mechanism\n",
    "\n",
    "This multi-headed attention layer operates slightly differently from the encoder one. Since the decoder is __autoregressive__ and generates the sequence word by word, one need to __prevent it from conditioning to future tokens__. For example, when computing attention scores on the word \"am\", one should not have access to the word \"fine\", because that word is a future word that was generated after. The word \"am\" should only have access to itself and the words before it. \n",
    "\n",
    "__This is true for all other words, where they can only attend to previous words.__\n",
    "\n",
    "So, when Ross says...\n",
    "<img src='./source/ross_is_fine.jpg' width=300 height=300>\n",
    "... he IS fine!\n",
    "\n",
    "<img src='./source/decoder_first_attention_1.png'>\n",
    "\n",
    "__*So, how do we prevent computing attention scores for future words?*__\n",
    "\n",
    "<img src='./source/transformer_self_attn.png' width=300 height=300>\n",
    "\n",
    "This is done using __Look-Ahead Mask__. The mask is a matrix that has the same size as the attention scores filled with __values of 0â€™s and negative infinities__. When you add the mask to the __scaled attention scores__, you get a matrix of the scores, with the top right triangle filled with negativity infinities.\n",
    "\n",
    "<img src='./source/decoder_first_attention_2.png'>\n",
    "\n",
    "Once you take the __`softmax`__ of the masked scores, the negative infinities becomes 0, leaving a zero attention scores for future tokens.\n",
    "\n",
    "<img src='./source/decoder_first_attention_3.png'>\n",
    "\n",
    "This component also has __multiple heads__, in each of them the mask is being applied and then getting concatenated. Again, an example for clear understanding.\n",
    "\n",
    "<img src='./source/decoder-self-attention-example.png'>\n",
    "\n",
    "Then similar to the encoder, the model employ residual connections followed by layer normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Multi-Headed Attention Mechanism\n",
    "\n",
    "For this layer, the inputs are:\n",
    "* <span style='color:purple'><b>Query</b></span> - output of the masked multi-headed attention layer of decoder\n",
    "* <span style='color:orange'><b>Key</b></span> - Encoder's output\n",
    "* <span style='color:blue'><b>Value</b></span> - Encoder's output\n",
    "\n",
    "This process matches the encoder's output to the decoder's output, allowing the decoder to decide __which encoder section is relevant to put a focus on__. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output.\n",
    "\n",
    "Hence, this layer is also called __encoder-decoder attention__ or __source-target attention__. The following picture will help you to understand this.\n",
    "\n",
    "<img src='./source/self_attn-enc_dec_attn-difference.png' width=750 height=750>\n",
    "\n",
    "An example to understand it better.\n",
    "\n",
    "<img src='./source/encoder-decoder-self-attention-example.png'>\n",
    "\n",
    "Then, again, the model performs residual connection followed by layer normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Neural Network\n",
    "\n",
    "Just like in Encoder, the output of encoder-decoder attention is fed to a FFNN to process it in an acceptable form to be fed to the final layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifier and Softmax Function\n",
    "\n",
    "The decoder stack outputs a vector of floats. How do we turn that into a word? That is the job of the final __Linear layer__ which is followed by a __`softmax` layer__.\n",
    "\n",
    "* The __Linear layer__ is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a __logits vector__. So this layer is basically a __classifier__. The classifier is as big as the number of classes you have. With respect to this paper, this layer ~30,000 classes for ~30,000 words. This would make the logits vector ~30,000 cells wide â€“ each cell corresponding to the score of a unique word.\n",
    "* The __`softmax` layer__ then turns those scores into probabilities _(all positive, all add up to 1.0)_. The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n",
    "\n",
    "<img src='./source/transformer_decoder_output_softmax.png' width=600 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function\n",
    "\n",
    "Authors of the paper used __Adam__ optimizer with a __custom learning rate__ that varied over the course of training. This is achieved using the formula:\n",
    "\n",
    "<img src=\"./source/optimizer.png\" width=500>\n",
    "\n",
    "where `warmup_steps` = 4000.\n",
    "\n",
    "As for loss function, the paper is using __Categorical Cross Entropy__.\n",
    "\n",
    "<img src=\"./source/transformer_logits_output_and_label.png\" width=500>\n",
    "\n",
    "\n",
    "### Final view of the decoder output\n",
    "\n",
    "If we go back to the translation example, the output from the decor will be as follows:\n",
    "\n",
    "__Ground Truth__\n",
    "<img src=\"./source/output_target_probability_distributions.png\" width=500>\n",
    "\n",
    "__Predicted Answer__\n",
    "<img src=\"./source/output_trained_model_probability_distributions.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:brown; font-size:150%;'><b>Decoder - Wrap up!</b></span>\n",
    "\n",
    "That wraps up the decoder layer. Now, the decoder will be able to map the relevant information with the encoder output, capturing the context and generating the result. You can __stack the decoder `N times`__ just like it was done in encoder to further process and decode the information, where each layer has the opportunity to learn different attention representations therefore potentially boosting the predictive power of the transformer network.\n",
    "\n",
    "__The OpenAI GPT-2 model uses these decoder-only blocks.__ Here is a sample output of GPT-2.\n",
    "\n",
    "<img src='./source/gpt-2-autoregression-2.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤— Transformers - Wrap up!\n",
    "\n",
    "So, we covered individually, how each components in Encoder and Decoder works. Let's take a look at how they work together.\n",
    "\n",
    "For simplicity, we have taken a stack of 2 encoders and decoders and performing French-to-English translation.\n",
    "\n",
    "<img src='./source/transformer_decoding_1.gif'>\n",
    "\n",
    "After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence.\n",
    "\n",
    "The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\n",
    "\n",
    "This step is repeated till the model spits out `<eos>` token indicating the end of process (here, translation).\n",
    "\n",
    "<img src='./source/transformer_decoding_2.gif'>\n",
    "\n",
    "That's it! This is the entire mechanics of the transformers.\n",
    "\n",
    "Now, it would be easier for you to go through the original paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf).\n",
    "\n",
    "Take a look at how [TensorFlow](https://www.tensorflow.org/tutorials/text/transformer) have implemented with code snippets for even more detailed understanding of the model.\n",
    "\n",
    "<br>\n",
    "<details><summary>But for Joey... </summary>\n",
    "    <img src='./source/joey_french.png'>\n",
    "    <p>Looks like we can train a model to translate but Joey is impossible. So all we can say is...</p>\n",
    "    <img src='./source/good-job-joey.jpg' width=500>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <span style='color:blue; font-size:200%;'><br><b>La Fin.</b></span>\n",
    "    </div>\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "588.997px",
    "left": "114px",
    "top": "199.573px",
    "width": "300.573px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
